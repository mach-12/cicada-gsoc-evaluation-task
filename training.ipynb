{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook\n",
    "\n",
    "Authored by [Mann Acharya](https://github.com/mach-12/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorboard_trails.png](plots/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this solution notebook we focus on these key tasks:\n",
    "\n",
    "- Select and implement a CNN architecture using Keras.\n",
    "- Perform hyperparameter tuning with Optuna for optimal model fit.\n",
    "- Set up TensorBoard for experiment tracking.\n",
    "- Save the best-performing model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Loading Dataset](#loading-dataset)\n",
    "3. [Model Architecture](#model-architecture)\n",
    "4. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "   - [Running Script](#running-script)\n",
    "5. [Viewing Results using TensorBoard](#viewing-results-using-tensorboard)\n",
    "   - [Best Parameters](#best-parameters)\n",
    "6. [Dumping Best Hyper-parameters](#dumping-best-hyper-parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "\n",
    "import optuna\n",
    "from optuna_integration import TensorBoardCallback\n",
    "from scripts.model import AutoencoderModel, ClassifierModel\n",
    "from scripts.dataset_loader import TrainingDatasetLoader\n",
    "\n",
    "from scripts.model_plots import create_neural_network_visualization\n",
    "\n",
    "from scripts.utils import save_params_to_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TrainingDatasetLoader(batch_size=32, test_size=0.2, seed=200)\n",
    "(train_dataset, test_dataset, images_train, images_test, labels_train, labels_test) = (\n",
    "    loader.load_training_dataset()\n",
    ")\n",
    "\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the `AutoencoderModel`, and there is also a `ClassifierModel`. The approach is inspired by the general workings and lessons from building a Variational Autoencoder. Let's see how the pipeline works:\n",
    "\n",
    "1. A `16x16` image enters the `AutoencoderModel`.\n",
    "\n",
    "2. **Encoding Phase:** We use a **2D convolution**, doubling the number of filters while keeping the same kernel size. This means the dimensions of the output image are reduced by half, while the number of features doubles, effectively representing the same image with more abstract features.\n",
    "\n",
    "3. We then apply a **downsampling operation** using the `MaxPooling2D` layer, which allows only the most prominent features to pass through.\n",
    "4. The output is **flattened** and passed through a **latent layer**, which captures the core features of the data that matter most. A **dropout layer** is added to prevent overfitting when constructing this latent space. Through experiments, we determine the best value for dropout.\n",
    "\n",
    "5. We **upsample and reconstruct** the image back to `16x16` using `Conv2DTranspose` filters. A **sigmoid activation** is applied at the end, returning the final image embeddings.\n",
    "\n",
    "6. These **image embeddings** are then passed into the `ClassifierModel`, which consists of **8 dense units** with a **softmax activation**. This model classifies the image into two categories:\n",
    "   - **Label 0:** Background\n",
    "   - **Label 1:** Signal\n",
    "\n",
    "All of these **hyperparameters**—`filters`, `latent_dim`, `kernel_size`, and `dropout_rate`—will be **tuned and tested** to find the optimal values!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![autoencoder_architecture.png](plots/autoencoder_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture Image saved to './plots/autoencoder_architecture.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/visualkeras/layered.py:86: UserWarning: The legend_text_spacing_offset parameter is deprecated and will be removed in a future release.\n",
      "  warnings.warn(\"The legend_text_spacing_offset parameter is deprecated and will be removed in a future release.\")\n"
     ]
    }
   ],
   "source": [
    "test_autoencoder_model = AutoencoderModel(\n",
    "    input_shape=(16, 16, 1), filters=16, latent_dim=16, kernel_size=3, dropout_rate=0.0\n",
    ").get_model()\n",
    "\n",
    "\n",
    "create_neural_network_visualization(test_autoencoder_model, name=\"autoencoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define an Optuna Objective to store suggestions for our tuning study\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    filters = trial.suggest_int(\"filters\", 16, 64, step=16)\n",
    "    latent_dim = trial.suggest_int(\"latent_dim\", 16, 64, step=16)\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 3, 5)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # Build the autoencoder and classifier using the suggested hyperparameters\n",
    "    autoencoder = AutoencoderModel(\n",
    "        input_shape=(16, 16, 1),\n",
    "        filters=filters,\n",
    "        latent_dim=latent_dim,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "    ).get_model()\n",
    "\n",
    "    encoder = Model(autoencoder.input, autoencoder.get_layer(\"latent\").output)\n",
    "\n",
    "    classifier_model = ClassifierModel(encoder, NUM_CLASSES)\n",
    "\n",
    "    # Compile the model with the suggested learning rate\n",
    "    optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    classifier_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    epochs = 100\n",
    "    history = classifier_model.fit(\n",
    "        train_dataset, epochs=epochs, validation_data=test_dataset, verbose=0\n",
    "    )\n",
    "\n",
    "    # Return the best validation accuracy as the objective value\n",
    "    best_val_acc = max(history.history[\"val_accuracy\"])\n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/q2yb2fpj1h36gmjgd793x7_w0000gn/T/ipykernel_9137/1024502627.py:3: ExperimentalWarning: TensorBoardCallback is experimental (supported from v2.0.0). The interface can change in the future.\n",
      "  tensorboard_callback = TensorBoardCallback(\"logs/\", metric_name=\"accuracy\")\n",
      "[I 2025-03-04 21:10:41,801] A new study created in memory with name: no-name-6e3be784-3f58-4b28-8152-280cad7768c8\n",
      "[I 2025-03-04 21:10:47,548] Trial 0 finished with value: 1.0 and parameters: {'filters': 32, 'latent_dim': 64, 'kernel_size': 3, 'dropout_rate': 0.1, 'learning_rate': 0.0009820466966219334}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:10:52,088] Trial 1 finished with value: 1.0 and parameters: {'filters': 16, 'latent_dim': 64, 'kernel_size': 3, 'dropout_rate': 0.5, 'learning_rate': 0.00039922905080119536}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:11:01,041] Trial 2 finished with value: 1.0 and parameters: {'filters': 32, 'latent_dim': 64, 'kernel_size': 5, 'dropout_rate': 0.5, 'learning_rate': 0.0006189829846978663}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:11:06,750] Trial 3 finished with value: 1.0 and parameters: {'filters': 32, 'latent_dim': 48, 'kernel_size': 3, 'dropout_rate': 0.0, 'learning_rate': 0.007898179832326341}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:11:15,480] Trial 4 finished with value: 1.0 and parameters: {'filters': 32, 'latent_dim': 64, 'kernel_size': 5, 'dropout_rate': 0.4, 'learning_rate': 0.002757944722969225}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:11:22,046] Trial 5 finished with value: 0.925000011920929 and parameters: {'filters': 32, 'latent_dim': 16, 'kernel_size': 4, 'dropout_rate': 0.5, 'learning_rate': 0.004813260255873197}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:11:26,145] Trial 6 finished with value: 0.9399999976158142 and parameters: {'filters': 16, 'latent_dim': 16, 'kernel_size': 3, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00017915474095984025}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:11:31,460] Trial 7 finished with value: 0.9900000095367432 and parameters: {'filters': 32, 'latent_dim': 16, 'kernel_size': 3, 'dropout_rate': 0.5, 'learning_rate': 0.0024985813265246564}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:11:46,053] Trial 8 finished with value: 1.0 and parameters: {'filters': 48, 'latent_dim': 32, 'kernel_size': 5, 'dropout_rate': 0.2, 'learning_rate': 0.002445958023587074}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:12:01,534] Trial 9 finished with value: 0.9599999785423279 and parameters: {'filters': 64, 'latent_dim': 16, 'kernel_size': 4, 'dropout_rate': 0.4, 'learning_rate': 0.004018197388622115}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:12:17,561] Trial 10 finished with value: 0.9649999737739563 and parameters: {'filters': 64, 'latent_dim': 48, 'kernel_size': 4, 'dropout_rate': 0.0, 'learning_rate': 0.00010100006808758547}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:12:22,529] Trial 11 finished with value: 0.9950000047683716 and parameters: {'filters': 16, 'latent_dim': 64, 'kernel_size': 3, 'dropout_rate': 0.1, 'learning_rate': 0.0006165480455578889}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:12:27,027] Trial 12 finished with value: 0.9850000143051147 and parameters: {'filters': 16, 'latent_dim': 48, 'kernel_size': 3, 'dropout_rate': 0.2, 'learning_rate': 0.0003357417472049742}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:12:35,163] Trial 13 finished with value: 1.0 and parameters: {'filters': 48, 'latent_dim': 64, 'kernel_size': 3, 'dropout_rate': 0.1, 'learning_rate': 0.0011941559465917834}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:12:39,498] Trial 14 finished with value: 0.9850000143051147 and parameters: {'filters': 16, 'latent_dim': 32, 'kernel_size': 4, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.001085803649289821}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:12:47,494] Trial 15 finished with value: 0.9900000095367432 and parameters: {'filters': 48, 'latent_dim': 48, 'kernel_size': 3, 'dropout_rate': 0.1, 'learning_rate': 0.0003500455957607461}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:12:52,146] Trial 16 finished with value: 1.0 and parameters: {'filters': 16, 'latent_dim': 64, 'kernel_size': 4, 'dropout_rate': 0.2, 'learning_rate': 0.00047508478386834135}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:12:56,497] Trial 17 finished with value: 0.9950000047683716 and parameters: {'filters': 16, 'latent_dim': 64, 'kernel_size': 3, 'dropout_rate': 0.4, 'learning_rate': 0.00019897872503108415}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:13:03,433] Trial 18 finished with value: 1.0 and parameters: {'filters': 32, 'latent_dim': 32, 'kernel_size': 4, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0013451180354540828}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-03-04 21:13:11,535] Trial 19 finished with value: 1.0 and parameters: {'filters': 48, 'latent_dim': 48, 'kernel_size': 3, 'dropout_rate': 0.1, 'learning_rate': 0.0008473682954502737}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Best Validation Accuracy:  1.0\n",
      "  Best Hyperparameters:\n",
      "    filters: 32\n",
      "    latent_dim: 64\n",
      "    kernel_size: 3\n",
      "    dropout_rate: 0.1\n",
      "    learning_rate: 0.0009820466966219334\n"
     ]
    }
   ],
   "source": [
    "# Set up the TensorBoard callback from Optuna\n",
    "# Referred from: https://github.com/optuna/optuna-examples/blob/main/tensorboard/tensorboard_simple.py\n",
    "tensorboard_callback = TensorBoardCallback(\"logs/\", metric_name=\"accuracy\")\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=20,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(\"  Best Validation Accuracy: \", best_trial.value)\n",
    "print(\"  Best Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Results using TensorBoard\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can launch **TensorBoard** for exploring logs of the experimentation.\n",
    "\n",
    "To launch, Use the inline magic function\n",
    "\n",
    "```bash\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "```\n",
    "\n",
    "OR, In the root of the repository run the below command\n",
    "\n",
    "````\n",
    "tensorboard --logdir logs```\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can the best parameters obtained in the training run and explore more in the dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorboard_trails.png](plots/tensorboard_trails.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorboard_best_params.png](plots/tensorboard_best_params.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping Best Hyper-parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are saving the optimal training configuration to a `.yaml` file, ensuring our experiments are easily reproducible across notebooks. This prevents us from scattering hardcoded values throughout our code, making it more maintainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_trial.params\n",
    "save_params_to_yaml(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
